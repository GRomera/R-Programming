---
title: 'Guillermo Romera | Practicum 3 | DA5030 | Intro to Data Mining/Machine Learning
  | Term  Fall 2017 '
output: html_notebook
---



Problem 1
(0 pts) Download the data set Bank Marketing Data Set. Note that the data file does not contain header names; you may wish to add those. The description of each column can be found in the data set explanation. This assignment must be completed within an R Markdown Notebook.

```{r warning=FALSE}
#Getting the data set to start working
library(readr)

bank <- read_delim("D:/R_TEst/MAchine Learning 2017/Practicum3/bank.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)

library(readr)
bank<- read_delim("D:/R_TEst/MAchine Learning 2017/Practicum3/bank-full.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)



```


(0 pts) Explore the data set as you see fit and that allows you to get a sense of the data and get comfortable with it. Is there distributional skew in any of the features? Is there a need to apply a transform? 

```{r}


#as usually we check if the data set has missing values, which it doesn't
sum(is.na(bank)) 

#As usually I like to create quite a few plots to visualize better the data
#I do create density plots to visualize the overall density and the histotgrams to see it more in detail

library(plyr)
library(psych)
par(mfrow=c(2,4))
d1 <- density(bank$age) 
plot(d1) 
d <- density(bank$balance) 
plot(d)
d <- density(bank$day) 
plot(d) 
d <- density(bank$duration) 
plot(d) 
d <- density(bank$campaign) 
plot(d) 
d <- density(bank$pdays) 
plot(d)
d <- density(bank$previous) 
plot(d)

par(mfrow=c(2,4))
x1 <- bank$age
h<-hist(x1, breaks=10, col="green", xlab="age", 
  	main="Histogram with Overlay Curve" )
xfit<-seq(min(x1),max(x1),length=40) 

x2 <- bank$balance
h<-hist(x2, breaks=10, col="green", xlab="balance", 
  	main="Histogram with Overlay Curve")
xfit<-seq(min(x2),max(x2),length=40) 

x3 <- bank$day
h<-hist(x3, breaks=10, col="green", xlab="day", 
  	main="Histogram with Overlay Curve")
xfit<-seq(min(x3),max(x3),length=40) 

x4 <- bank$duration
h<-hist(x4, breaks=10, col="green", xlab="duration", 
  	main="Histogram with Overlay Curve")
xfit<-seq(min(x4),max(x4),length=40) 

x5 <- bank$campaign
h<-hist(x5, breaks=10, col="green", xlab="campaign", 
  	main="Histogram with Overlay Curve")
xfit<-seq(min(x5),max(x5),length=40) 

x6 <- bank$pdays
h<-hist(x6, breaks=10, col="green", xlab="pdays", 
  	main="Histogram with Overlay Curve")
xfit<-seq(min(x6),max(x6),length=40) 

x7 <- bank$previous
h<-hist(x7, breaks=10, col="green", xlab="previous", 
  	main="Histogram with Overlay Curve")
xfit<-seq(min(x7),max(x7),length=40) 

#Conclusion
#After reviewing the data and running the algorithms before transforming any data I have seen that the results were very consistent
#Also while doing some research about this particular data set I've seen people did not really transformed the data, and some
#what they only did was removing the Duration column from the data set
#Since I have seen how well these algorithms (SVM,NEURAL NET) perform without the unaltered data and seeing other people's work
#I have decided not to transform the data as I don't seem there's any need for it, give the algorithms we are using
```

3. (20 pts) Build a classification model using a support vector machine that predicts if a bank customer will open a term deposit account.

```{r}
library(e1071)
library(caret)
library(klaR)

library(readr)
bank <- read_delim("D:/R_TEst/MAchine Learning 2017/Practicum3/bank.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)

table(bank$y)

library(kernlab) 
#We separate the original data set into two equal halves
btrain<- bank[1:2261,]
btest<- bank[2261:4521, ]
#We trainn the model on the trianing data set
svm1 <- ksvm(y ~ ., data = btrain,kernel = "vanilladot")

#We then make a prediction with the model created above on the test data set
svmp1 <- predict(svm1, btest) 

#basic table to see how well the algorithm has predicted the target
table(svmp1, btest$y) 

acc1 <- svmp1 == btest$y

#We create a table with the variable assigned before to see how many have been properly predicted
table(acc1) 
 
#This creates a basic accuracy table
prop.table(table(acc1)) 

#Basic confusion matrix that allow us to see the accuracy an some other information
confusionMatrix(svmp1, btest$y)


# We now try to improve the performance
svm2 <- ksvm(y ~ ., data = btrain, kernel = "rbfdot") 
 
#Again with predict with the new model created
svmp2  <- predict(svm2 ,btest) 


```

```{r}
####CARET LIBRARY METHOD####

library(e1071)
library(caret)
library(klaR)

library(readr)
bank <- read_delim("D:/R_TEst/MAchine Learning 2017/Practicum3/bank.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)

#Again as we did before we separate the data frame into two equal halves
btrain<- bank[1:2261,]
btest<- bank[2261:4521, ]

svm_model <- train(y~., data = btrain,
                   method = "svmPoly",
                   trControl= trainControl(method = "cv", number = 10),
                   tuneGrid = data.frame(degree = 1,scale = 1,C = 1))
svm_model
#We make a prediction running the model above for the test data set
SVMPredictions <-predict(svm_model, btest, na.action = na.pass)

#Basic confusion matrix that allow us to see the accuracy an some other information
confusionMatrix(SVMPredictions, btest$y)


#Conclusion
#It seems that the SVM algorithm gives us very strong results and high accuracy, so much without touching the data
#that most of my time spent with this problem was to make sure the results I was getting were correct
#As powerful as it is I see that the main issue this algorithm has is that it's limited by time and computing power constraints

```
4. (20 pts) Build another classification model using a neural network that also predicts if a bank customer will open a term deposit account.

```{r warning=FALSE}
####### CARET LIBRARY METHOD
library(readr)
bank <- read_delim("D:/R_TEst/MAchine Learning 2017/Practicum3/bank.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)

bank$duration<-NULL


btrain<- bank[1:2261,]
btest<- bank[2261:4521, ]

#I had to to this because the function outputted everything into the htlm file and it was overloading the memory, so with the invisible() and capture.output() it doesn't do that
invisible(capture.output(
nnmodel <- train(btrain[,-16], btrain$y, method = "nnet",
                 trControl= trainControl(method = "cv", number = 10))))


nnetpredictions <-predict(nnmodel, btest, na.action = na.pass)
confusionMatrix(nnetpredictions, btest$y)


acc3 <- nnetpredictions ==btest$y 
 
prop.table(table(acc3)) 
table(acc3) 
```


```{r}
####### NNET LIBRARY METHOD
library(readr)
bank <- read_delim("D:/R_TEst/MAchine Learning 2017/Practicum3/bank.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)

library(nnet)
bank$y <- as.numeric(bank$y == "no")

btrain1<- bank[1:2261,]
btest1<- bank[2261:4521, ]

invisible(capture.output(
model.nn <- nnet(y ~ ., data=btrain1, size=10, maxit=1000, decay=.01, linout=FALSE)))
preds.nn <- predict(model.nn, newdata=btest1, type="raw")


#Conclusion
#Same opinion as for the SVM algorithm, I still can't believe I'm getting this correct without any data manipulation at all,
#even after reading how powerful Neutal Networks are.
#On the other hand, Neural Nets seem even as a more demanding algorithm than SVM. I can see why other algorithms are wide spread
# instead of this, it requires quite a bit of time and computing power

```

5. (20 pts) Compare the accuracy of the two models based on absolute accuracy and AUC.

```{r}

library(ROCR)
library(pROC)



###Accuracy for the Support Vector Machine

#We again create a comparison between the original and the prediction
acc2 <- svmp2 ==btest$y 
 
#table to see how many cases has been properly predicted
table(acc2) 
 
#Table that allows us to see the accuracy of our prediction
prop.table(table(acc2)) 

#Basic confusion matrix that allow us to see the accuracy an some other information
confusionMatrix(svmp2, btest$y)

###AUC for the Support Vector Machine 
svmp2<-as.numeric(svmp2)
auc(btest$y,svmp2)

rocplot <- function(pred, truth, ...) {
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
  area <- auc(truth, pred)
  area <- format(round(area, 4), nsmall = 4)
  text(x=0.8, y=0.1, labels = paste("AUC =", area))

  
  segments(x0=0, y0=0, x1=1, y1=1, col="gray", lty=2)
}
#ROC for the Neural Network
rocplot(svmp2, btest$y, col="blue")






###Accuracy for the Neural Network
results.nn <- table(predicted=preds.nn, true=btest1$y)

pred.logit <- rep("0",length(preds.nn))
pred.logit[preds.nn>=0.5] <- "1"
confusionMatrix(btest1$y, pred.logit)


confusionMatrix(pred.logit, btest1$y)


acc4 <- pred.logit ==btest1$y 
 
prop.table(table(acc4)) 
table(acc4) 



#AUC for the Neural Network
auc(btest1$y,preds.nn)

rocplot <- function(pred, truth, ...) {
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
  area <- auc(truth, pred)
  area <- format(round(area, 4), nsmall = 4)
  text(x=0.8, y=0.1, labels = paste("AUC =", area))

 
  segments(x0=0, y0=0, x1=1, y1=1, col="gray", lty=2)
}
#ROC for the Neural Network
rocplot(preds.nn, btest1$y, col="blue")

```

Problem 2
(0 pts) Download the data set Plant Disease Data Set. Note that the data file does not contain header names; you may wish to add those. The description of each column can be found in the data set explanation. This assignment must be completed within an R Markdown Notebook.

```{r}




```

(0 pts) Explore the data set as you see fit and that allows you to get a sense of the data and get comfortable with it. Is there distributional skew in any of the features? Is there a need to apply a transform? 
```{r}





```
(40 pts) Use association rules to segment the data similar to what was done in Hämäläinen, W., & Nykänen, M. (2008, December). Efficient discovery of statistically significant association rules. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 203-212). IEEE.

```{r}
library(readr)
library(arules)
plants <- read.transactions("https://archive.ics.uci.edu/ml/machine-learning-databases/plants/plants.data", format = "basket",
                            sep = ",", cols = 1)

inspect(plants[1:5])


itemFrequencyPlot(plants, topN = 20) 


#we separate the data the same way it has been done on the paper
ptrain<-plants[1:15088]
ptest<-plants[15088:30176]

#We run the apriori() algorithm with the same parameters as they have done
rules1 <- apriori(ptrain, parameter = list(support = 0.12, confidence = 0.60, maxlen=70,target="rules"))
rules2 <- apriori(ptest, parameter = list(support = 0.12, confidence = 0.90, maxlen=70,target="rules"))
summary(rules1)
summary(rules2)

#write(rules1, file = "groceryrules1.csv", sep = ",", quote = TRUE, row.names = FALSE)
#This line save the grocery rules as a data frame in this R notebook
rules1df <- as(rules1, "data.frame") 
head(rules1df)
tail(rules1df)


#write(rules2, file = "groceryrules2.csv", sep = ",", quote = TRUE, row.names = FALSE)
#This line save the grocery rules as a data frame in this R notebook
rules2df <- as(rules2, "data.frame") 
head(rules2df)
tail(rules2df)


```

(30 pts) Are there clusters in the data? Can customers be segmented into groups? Build a k-means clustering model to investigate.
```{r}
library(dplyr)
library(plyr)


#The issue with this problem is that the data is in a transactional form so we need to convert that into a format
#that we can use the Kmeans clsutering algorithm. The most approapiate format is a Sparse Matrix
#The code below creates a Sparse Matrix out of the transaction form
#It does read each line of said data individually and the organizes it accordingly, in this case it converts the species into rows and the states into columns filing up with 0 or 1 when that particular specie is present in that state
fileName <- "https://archive.ics.uci.edu/ml/machine-learning-databases/plants/plants.data"
conn <- file(fileName,open="r")


linn <-readLines(conn)
entry<-strsplit(linn, "\n")
s <- c()

for (i in 1:length(entry))
{
  sub <- strsplit(entry[[i]], ",")
   s[[i]]<-sub[[1]][1]
}


t1 <- data.frame(species=s,
                 stringsAsFactors=FALSE) 


for (i in 1:length(entry))
  
{
   sub <- strsplit(entry[[i]], ",")
   
   states<-sub[[1]] [2 : length(sub[[1]])]
   newstates<-states[!(states %in% colnames(t1))]
   
   t1[newstates]<- 0
   
  for (j in 1:length(states))
  {
    statecolumn<-which(states[j]==colnames(t1))
    t1[i,statecolumn]<- 1
    
  }
  
     
}




library(klaR)
t2<-t1[1:1000,1:5]
t2$species<- seq(1,1000)

#Another issue with this problem is that since we are using a sparse matrix that leaves us with categorical data of 1 and 0 something that the regular Kmeans() algorithm can't using because it uses Euclidean Distance, hence why we need to use another distance calculation such as Hamming Distance. Hamming Distance has to be applied with the Kmode() function which makes clusters based on categorical data such as the one we have

#The running time as Prof.Schedlbauer mentioned it's quite high since clustering is a high demanding process, but it does work and tested with the parameteres I supplied
#In any case if we want to maximize the clustering the values provided below as a text can be used to seehow the data is clustered in more detail

cluster.results <-kmodes(t1[1:100,2:71],15) 
#cluster.results <-kmodes(t1[1:1000,2:71],15) 

t11<-t1[1:100,1:71]
#t11<-t1[1:1000,1:71]
cluster.output <- cbind(t11,cluster.results$cluster)
write.csv(cluster.output, file = "kmodes clusters.csv", row.names = TRUE)

```
(10 pts) Visualize the clusters.

```{r}

#Here I have decided to plotmultiple graphs to visualize the cluster since I was not entirely sure how to do it properly, and also has a fail safe measure.

library(ggplot2)
t3<-cbind.data.frame(cluster.output$species,cluster.output$`cluster.results$cluster`)
colnames(t3)<-c( "species", "clusters")
t3$species<- seq(1,100)
t4<-as.matrix(t3)

plot(jitter(t4), col = cluster.results$cluster)
points(cluster.results$modes, col = 1:2, pch = 8)



ggplot(cluster.output, aes(species, cluster.output$`cluster.results$cluster`, color = cluster.results$cluster)) + geom_point()


plot(cluster.results$cluster,col = cluster.results$cluster)


```
















